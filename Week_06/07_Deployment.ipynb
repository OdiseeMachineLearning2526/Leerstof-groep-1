{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In productie brengen van een ML model\n",
    "\n",
    "In deze notebook demonstreren we hoe je een machine learning-model in productie kunt brengen door het te serveren via een Flask API of het op te slaan in de cloud en lokaal uit te voeren. \n",
    "\n",
    "## Onderdelen:\n",
    "\n",
    "1. **Flask API Schrijven**:\n",
    "   - We schrijven een eenvoudige Flask API die een machine learning-model serveert.\n",
    "\n",
    "2. **Model Opslaan in de Cloud om lokaal uit te voeren**:\n",
    "   - We slaan het model op in GitHub zodat het toegankelijk is voor download.\n",
    "---\n",
    "\n",
    "## 1. Flask API Schrijven\n",
    "\n",
    "Voordat we beginnen met de code, laten we een korte uitleg geven over hoe de Flask API werkt.\n",
    "\n",
    "### Uitleg:\n",
    "\n",
    "- **Flask**: Een micro webframework voor Python dat het mogelijk maakt om snel en eenvoudig een webapplicatie te bouwen.\n",
    "- **API Endpoint**: We maken een endpoint (`/predict`) dat een afbeelding accepteert en voorspellingen doet met het getrainde model.\n",
    "\n",
    "Run deze file buiten de docker container en start de html via de live-server van visual studio code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting app.py\n"
     ]
    }
   ],
   "source": [
    "%%file app.py\n",
    "from flask import Flask, request, jsonify\n",
    "from flask_cors import CORS\n",
    "import torch\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import io\n",
    "\n",
    "app = Flask(__name__)\n",
    "CORS(app)\n",
    "\n",
    "model = models.resnet18(weights = models.ResNet18_Weights.DEFAULT)\n",
    "model.eval()\n",
    "\n",
    "dummy_inputs = torch.randn(1,3,224,224)\n",
    "torch.onnx.export(model, dummy_inputs, \"07_model.onnx\")\n",
    "\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "    print(request.files)\n",
    "    if 'file' not in request.files:\n",
    "        return jsonify({'error': 'No File part'}), 400\n",
    "\n",
    "    file = request.files['file']\n",
    "    if file.filename == '':\n",
    "        return jsonify({'error': 'No file selected'}), 400\n",
    "\n",
    "    print(file.filename)\n",
    "\n",
    "    # doe een voorspelling\n",
    "    try:\n",
    "        image = Image.open(io.BytesIO(file.read())) # lees de figuur\n",
    "        image = preprocess(image)\n",
    "        image = image.unsqueeze(0) # 3, B, H -> 1, 3, B, H\n",
    "        print(image.shape)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(image)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            result = jsonify({'class': predicted.item()})\n",
    "            print(result)\n",
    "            return result, 200\n",
    "    except Exception as e:\n",
    "        return jsonify({'error': str(e)}, 500)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(port=5000, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status code: 200\n",
      "Response JSON: {'class': 468}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = \"http://localhost:5000/predict\"  # your server & port\n",
    "file_path = \"img1.jpg\"  # change this to your file\n",
    "\n",
    "with open(file_path, \"rb\") as f:\n",
    "    files = {\"file\": (file_path.split(\"/\")[-1], f, \"image/jpg\")}\n",
    "    response = requests.post(url, files=files)\n",
    "\n",
    "print(\"Status code:\", response.status_code)\n",
    "print(\"Response JSON:\", response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model gebruiken in C#\n",
    "\n",
    "We kunnen ook de AI-modellen getrained in python gebruiken in applicaties die in een andere taal geschreven zijn. Hiervoor gaan we een C# applicatie maken die een model inlaad vanuit een bestand.\n",
    "Er zijn verschillende manieren om dit aan te pakken. Ofwel geef je het model mee bij de installatiefile ofwel plaats je het in een cloud-omgeving en gaat je applicatie het downloaden.\n",
    "Deze tweede aanpak is efficienter omdat het gemakkelijk is hiermee updates van het model te verspreiden. Wij gaan dit echter niet doen en houden het eenvoudig door het model hiervoor opgeslagen in te laden in de applicatie.\n",
    "\n",
    "Nu gaan we een C# application maken die dit model download en gebruikt.\n",
    "Hiervoor moeten we een console applicatie maken die gebruik maakt van de volgende nuget packages:\n",
    "* Microsoft.ML.OnnxRuntime\n",
    "* System.Drawing.Common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "d5e8e3a19af5ceb2434683dff87da6345c3b29f7eb0a8a138558c07d014a01cc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
