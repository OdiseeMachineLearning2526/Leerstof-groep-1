{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ece9ec2a-8afa-4f7f-bfe9-0609d1100f6b",
   "metadata": {},
   "source": [
    "# Classificatie van nieuwsartikelen\n",
    "\n",
    "In deze notebook gaan we verder werken op de AG-news nieuwsartikelen dataset.\n",
    "In de vorige notebook hebben we bekeken hoe we tekstuele data kunnen preprocessen.\n",
    "In deze notebook gaan we classificatie uitvoeren door gebruik te maken van recurrente neurale netwerken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2c9bcc4b-58d8-4a88-ad47-8557ab537b3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[5] * 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9f7c6d37-e785-4177-9e0d-8cebff1cbd4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wall St. Bears Claw Back Into the Black (Reute...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Carlyle Looks Toward Commercial Aerospace (Reu...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Oil and Economy Cloud Stocks' Outlook (Reuters...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Iraq Halts Oil Exports from Main Southern Pipe...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Oil prices soar to all-time record, posing new...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  Wall St. Bears Claw Back Into the Black (Reute...      2\n",
       "1  Carlyle Looks Toward Commercial Aerospace (Reu...      2\n",
       "2  Oil and Economy Cloud Stocks' Outlook (Reuters...      2\n",
       "3  Iraq Halts Oil Exports from Main Southern Pipe...      2\n",
       "4  Oil prices soar to all-time record, posing new...      2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of vocabulary 20002\n",
      "torch.Size([1000, 50])\n",
      "torch.Size([1000, 50])\n",
      "torch.Size([64, 50]) torch.Size([64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_118/420127414.py:76: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(self.texts[idx], dtype=torch.long), torch.tensor(self.labels[idx], dtype=torch.long)\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import kagglehub\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "MAX_NUM_WORDS = 20000\n",
    "MAX_SEQUENCE_LENGTH = 50\n",
    "EMBEDDING_DIM = 60\n",
    "\n",
    "# Load the dataset\n",
    "path = kagglehub.dataset_download(\"amananandrai/ag-news-classification-dataset\")\n",
    "\n",
    "def read_csv(filepath):\n",
    "    df = pd.read_csv(filepath)\n",
    "    df['text'] = df['Title'] + ' ' + df['Description']\n",
    "    df['label'] = df['Class Index'] - 1\n",
    "    return df[['text', 'label']]\n",
    "\n",
    "df_train = read_csv(f'{path}/train.csv')\n",
    "df_test = read_csv(f'{path}/test.csv')\n",
    "\n",
    "display(df_train.head())\n",
    "\n",
    "# Tokenizer\n",
    "def simple_tokenizer(text):\n",
    "    return re.findall(r\"\\b\\w+\\b\", text.lower())\n",
    "\n",
    "# tel hoeveel keer elk woord voorkomt\n",
    "counter = Counter()\n",
    "for text in df_train['text']:\n",
    "    counter.update(simple_tokenizer(text))\n",
    "\n",
    "# maak een woordenboek aan\n",
    "vocab = {word: idx + 2 for idx, (word, _) in enumerate(counter.most_common(MAX_NUM_WORDS))}\n",
    "vocab['<pad>'] = 0 # extra token voor padding\n",
    "vocab['<unk>'] = 1 # extra token voor onbekende woorden\n",
    "\n",
    "print('Size of vocabulary', len(vocab))\n",
    "\n",
    "# zet elk woord om naar een getal\n",
    "def encode(text, vocab, max_len):\n",
    "    tokens = simple_tokenizer(text) # text -> woorden\n",
    "    idxs = [vocab.get(token, vocab['<unk>']) for token in tokens] # elk woord -> idx\n",
    "    idxs = idxs[:max_len] # truncating\n",
    "    idxs += [vocab['<pad>']] * (max_len - len(idxs)) # padding\n",
    "    return idxs\n",
    "\n",
    "num_samples = 1000\n",
    "X_train = torch.tensor([encode(text, vocab, MAX_SEQUENCE_LENGTH) for text in df_train['text'][:num_samples]])\n",
    "y_train = df_train['label'][:num_samples]\n",
    "print(X_train.shape)\n",
    "\n",
    "X_test = torch.tensor([encode(text, vocab, MAX_SEQUENCE_LENGTH) for text in df_test['text'][:num_samples]])\n",
    "y_test = df_test['label'][:num_samples]\n",
    "print(X_test.shape)\n",
    "\n",
    "# Dataset + dataloader\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        super(TextDataset, self).__init__()\n",
    "        \n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.texts[idx], dtype=torch.long), torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "\n",
    "train_dataset = TextDataset(X_train, y_train)\n",
    "test_dataset = TextDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "t, l = next(iter(train_loader))\n",
    "\n",
    "print(t.shape, l.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fdd6c70-ace6-4e59-8973-b6a60f61fadf",
   "metadata": {},
   "source": [
    "## Opbouwen, trainen en evalueren van een RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2e78be1a-0354-4a66-9936-770c68c9b604",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 50]) torch.Size([64])\n",
      "torch.Size([64, 4])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_118/420127414.py:76: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(self.texts[idx], dtype=torch.long), torch.tensor(self.labels[idx], dtype=torch.long)\n"
     ]
    }
   ],
   "source": [
    "# RNN model\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
    "        super(RNNModel, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim, num_layers=5, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.embedding(x)\n",
    "        outputs, hidden = self.rnn(x)\n",
    "        # wil je een sequentie als output -> werk met de outputs\n",
    "        # wil je enkel classificatie doen op de hele tekst -> dan kan je met de hidden werken of je selecteert de laatste output\n",
    "        x = hidden[0]\n",
    "        return self.fc(x)\n",
    "\n",
    "model = RNNModel(len(vocab), EMBEDDING_DIM, 100, 4)\n",
    "\n",
    "t, l = next(iter(train_loader))\n",
    "outputs = model(t)\n",
    "\n",
    "print(t.shape, l.shape)\n",
    "print(outputs.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "08f036a6-5932-4948-aa2f-ce4129738238",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_118/420127414.py:76: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(self.texts[idx], dtype=torch.long), torch.tensor(self.labels[idx], dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, loss: 0.08051504194736481\n",
      "Epoch 2/20, loss: 0.08106335997581482\n",
      "Epoch 3/20, loss: 0.0706050917506218\n",
      "Epoch 4/20, loss: 0.07702729851007462\n",
      "Epoch 5/20, loss: 0.07443859428167343\n",
      "Epoch 6/20, loss: 0.06988861411809921\n",
      "Epoch 7/20, loss: 0.062111109495162964\n",
      "Epoch 8/20, loss: 0.051068197935819626\n",
      "Epoch 9/20, loss: 0.06091461330652237\n",
      "Epoch 10/20, loss: 0.06228755787014961\n",
      "Epoch 11/20, loss: 0.042413994669914246\n",
      "Epoch 12/20, loss: 0.046234361827373505\n",
      "Epoch 13/20, loss: 0.04114314168691635\n",
      "Epoch 14/20, loss: 0.05761498212814331\n",
      "Epoch 15/20, loss: 0.057566843926906586\n",
      "Epoch 16/20, loss: 0.033861447125673294\n",
      "Epoch 17/20, loss: 0.0342138446867466\n",
      "Epoch 18/20, loss: 0.0481715127825737\n",
      "Epoch 19/20, loss: 0.03751064091920853\n",
      "Epoch 20/20, loss: 0.036112114787101746\n"
     ]
    }
   ],
   "source": [
    "# Train het Model\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.002)\n",
    "\n",
    "num_epochs = 20\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0\n",
    "    for texts, labels in train_loader:\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(texts)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss = loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch +1}/{num_epochs}, loss: {running_loss/len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9eb57af5-0ff0-499f-94be-35e7e5ea89e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total: 1000 correcte: 261 acc: 0.261\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_118/420127414.py:76: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(self.texts[idx], dtype=torch.long), torch.tensor(self.labels[idx], dtype=torch.long)\n"
     ]
    }
   ],
   "source": [
    "# Evalueer het Model\n",
    "\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for texts, labels in test_loader:\n",
    "        outputs = model(texts)\n",
    "        _, predictions = torch.max(outputs, dim=1)\n",
    "\n",
    "        total += len(labels)\n",
    "        correct += (predictions == labels).sum().item()\n",
    "\n",
    "print('total:', total, 'correcte:', correct, 'acc:', correct/total)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6277e4fc-e559-4f43-8d88-1840cc31dc52",
   "metadata": {},
   "source": [
    "## Oefeningen\n",
    "\n",
    "* Voeg een extra Dense-laag toe na de RNN-laag. Experimenteer met het aantal neuronen in deze laag en analyseer hoe de prestaties veranderen.\n",
    "* Pas het model aan om in plaats van een SimpleRNN-laag een LSTM of GRU-laag te gebruiken. Vergelijk de prestaties van de drie typen recurrente netwerken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8991aec0-9549-4d1b-99fb-897d2331fb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oefening 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7775c80b-0838-4a41-848a-25c499141be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oefening 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0a7182-6e98-40a4-953d-3051cb8687a9",
   "metadata": {},
   "source": [
    "**Oefening 3**\n",
    "\n",
    "Volg de tutorial op de volgende link: https://www.tensorflow.org/text/tutorials/text_generation\n",
    "Werk hieronder het gelijkaardige probleem uit maar maak het door gebruik te maken van pytorch in plaats van tensorflow voor het model op te bouwen.\n",
    "In deze tutorial wordt er tekst gegenereerd die lijkt op tekst geschreven door shakespeare.\n",
    "Let op dat dit een vereenvoudigde versie is waarbij karakter per karakter wordt gegenereerd en niet woord per woord. Er is dus geen garantie dat er echte woorden gemaakt worden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edab02f2-fa9f-4306-8ad2-a378b45eb9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "import random\n",
    "\n",
    "path_to_file = keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
    "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
    "print(f'Length of text: {len(text)} characters')\n",
    "print(text[:250])\n",
    "# The unique characters in the file\n",
    "vocab = sorted(set(text))\n",
    "print(f'{len(vocab)} unique characters')\n",
    "\n",
    "# Character to index mapping\n",
    "char_to_idx = {char: idx for idx, char in enumerate(vocab)}\n",
    "idx_to_char = {idx: char for idx, char in enumerate(vocab)}\n",
    "\n",
    "# TODO: Encodeer elk karakter in tekst naar een nummer, uitkomst is een list ipv een string\n",
    "\n",
    "# TODO: Maak een dataset aan waarbij de tekst (uit voorgaande todo) omzet naar een reeks sequenties\n",
    "# input 100 aaneensluitende karakters, output is het karakter erop volgende\n",
    "\n",
    "# TODO: indien nodig maak een subset tot 10 of 1% van de dataset\n",
    "\n",
    "# Check a single example\n",
    "sample_x, sample_y = dataset[0]\n",
    "print(\"Input (x):\", sample_x)\n",
    "print(\"Target (y):\", sample_y)\n",
    "print(\"Decoded Input:\", ''.join(idx_to_char[idx] for idx in sample_x.numpy()))\n",
    "print(\"Decoded Target:\", idx_to_char[sample_y.item()])\n",
    "print('Rows', len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910b060e-a61f-4a32-9dcd-ba625ebc222c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Maak een rnn model bestaande uit een embedding layer, gru layer en linear layer\n",
    "# Maak het mogelijk om aan de forward funtie een parameter toe te voegen om ook de hidden state terug te geven en om de hidden state mee te geven voor de gru laag\n",
    "# \n",
    "# TODO: Maak een rnn model bestaande uit een embedding layer, gru layer en linear layer\n",
    "# Maak het mogelijk om aan de forward funtie een parameter toe te voegen om ook de hidden state terug te geven en om de hidden state mee te geven voor de gru laag\n",
    "# \n",
    "vocab_size = len(idx_to_char)\n",
    "print(vocab_size)\n",
    "embedding_dim = 50\n",
    "rnn_units = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd60acb-29d3-4087-b7a9-8f64b2c9fd64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test 1 sample om door het model te sturen\n",
    "# kijk of je dimensies correct aan elkaar gekoppeld zijn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45fbb345-d9f0-477a-98c2-2aa83db41963",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import math\n",
    "\n",
    "batch_size = 64\n",
    "seq_length = 100\n",
    "epochs = 5\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = 50\n",
    "rnn_units = 60\n",
    "\n",
    "shakespeare = ShakespeareModel(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec92677b-7f6b-4cb1-9f6b-faf2da8a68b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def generate_text(model, start_string, char_to_idx, idx_to_char, vocab_size, generation_length=100, temperature=1.0):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    \n",
    "    # Convert start_string to indices\n",
    "    input_indices = torch.tensor([char_to_idx[char] for char in start_string], dtype=torch.long).unsqueeze(0)\n",
    "    \n",
    "    generated_text = start_string\n",
    "    states = None  # Initial state (None means it will be initialized automatically)\n",
    "    \n",
    "    for _ in range(generation_length):\n",
    "        # Genereer opeenvolgend nieuwe tokens\n",
    "        pass\n",
    "    \n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d49248-9fad-4174-a5a1-af319bcfc38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example start string and generation parameters\n",
    "start_string = \"ROMEO: \"\n",
    "generation_length = 200\n",
    "temperature = 0.8\n",
    "\n",
    "# Generate text\n",
    "generated_text = generate_text(\n",
    "    model=shakespeare,\n",
    "    start_string=start_string,\n",
    "    char_to_idx=char_to_idx,\n",
    "    idx_to_char=idx_to_char,\n",
    "    vocab_size=vocab_size,\n",
    "    generation_length=generation_length,\n",
    "    temperature=temperature\n",
    ")\n",
    "\n",
    "print(\"Generated Text:\")\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645098d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5489145",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
