{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7c1b610-2621-4f40-8e1a-e92da92e5c08",
   "metadata": {},
   "source": [
    "# Transformer Model for Translation using PyTorch DIT MOET JE NIET ZELF KUNNEN CODEREN/TER INFORMATIE\n",
    "\n",
    "In deze notebook zullen we een Transformer-model implementeren voor vertalingen. We zullen gebruik maken van PyTorch. We zullen de volgende onderdelen behandelen:\n",
    "\n",
    "1. **Inleiding tot Transformer Architectuur**\n",
    "2. **Data Voorbereiding**\n",
    "3. **Model Architectuur**\n",
    "4. **Training en Evaluatie**\n",
    "5. **Voorspellingen**\n",
    "\n",
    "## 1. Inleiding tot Transformer Architectuur\n",
    "\n",
    "De Transformer is een type neurale netwerkarchitectuur die gebruik maakt van zelf-attentie mechanismen om sequentiële data te verwerken, zoals tekst voor vertalingen. De belangrijkste onderdelen van een Transformer zijn:\n",
    "\n",
    "- **Encoder**: Neemt de bronzin en verwerkt deze in een reeks van contextuele representaties.\n",
    "- **Decoder**: Neemt de contextuele representaties van de encoder en genereert de doeltalige zin.\n",
    "- **Self-Attention Mechanism**: Stelt het model in staat om aandacht te besteden aan verschillende delen van de invoer tijdens de verwerking.\n",
    "\n",
    "## 2. Data Voorbereiding\n",
    "\n",
    "We zullen een voorbeeld dataset gebruiken voor vertalingen. Voor dit voorbeeld gebruiken we een kleine dataset van Engelse naar Franse zinnen. We zullen de data tokeniseren en omzetten naar numerieke waarden.\n",
    "\n",
    "Hierbij worden de volgende stappen uitgevoerd:\n",
    "* Tokenisatie: Split de tekst in woorden en zet deze om naar numerieke indices.\n",
    "* Padding: Zorg ervoor dat alle zinnen dezelfde lengte hebben door padding toe te voegen.\n",
    "* DataLoader: Zorgt voor batching en shuffling van data tijdens training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82f3608-e8b4-4038-8188-c8da28f1aca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importeer de benodigde libraries\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from collections import Counter\n",
    "from timeit import default_timer as timer\n",
    "import os\n",
    "import kagglehub\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Download en laad de dataset\n",
    "path = kagglehub.dataset_download(\"devicharith/language-translation-englishfrench\")\n",
    "\n",
    "filename = 'eng_-french.csv'\n",
    "df = pd.read_csv(f\"{path}/{filename}\")\n",
    "df_train = df.sample(frac=0.8)\n",
    "df_test = df.drop(df_train.index)\n",
    "\n",
    "# Bekijk een paar voorbeelden van de data\n",
    "print(df.head())\n",
    "\n",
    "# Special tokens\n",
    "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
    "special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
    "\n",
    "def tokenize(text):\n",
    "    return text.lower().split()\n",
    "\n",
    "def build_vocab(sentences):\n",
    "    counter = Counter()\n",
    "    for sentence in sentences:\n",
    "        counter.update(tokenize(sentence))\n",
    "    vocab = special_symbols + list(counter.keys())\n",
    "    return {word: idx for idx, word in enumerate(vocab)}\n",
    "\n",
    "def sentence_to_tensor(sentence, vocab):\n",
    "    indices = [vocab.get(word, UNK_IDX) for word in tokenize(sentence)]\n",
    "    return torch.tensor(indices, dtype=torch.long)\n",
    "\n",
    "# Collate function for batching\n",
    "def collate_fn(batch):\n",
    "    src_batch = [sentence_to_tensor(item['english'], source_vocab) for item in batch]\n",
    "    tgt_batch = [sentence_to_tensor(item['french'], target_vocab) for item in batch]\n",
    "\n",
    "    # Pad sequences\n",
    "    src_batch = pad_sequence(src_batch, batch_first=False, padding_value=PAD_IDX)\n",
    "    tgt_batch = pad_sequence(tgt_batch, batch_first=False, padding_value=PAD_IDX)\n",
    "\n",
    "    # Add BOS and EOS\n",
    "    bos_tensor = torch.full((1, src_batch.size(1)), BOS_IDX)  # [1, batch]\n",
    "    eos_tensor = torch.full((1, src_batch.size(1)), EOS_IDX)  # [1, batch]\n",
    "\n",
    "    src_batch = torch.cat((bos_tensor, src_batch, eos_tensor), dim=0)\n",
    "    tgt_batch = torch.cat((bos_tensor, tgt_batch, eos_tensor), dim=0)\n",
    "\n",
    "    return src_batch, tgt_batch\n",
    "\n",
    "# Dataset wrapper\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.dataframe = dataframe\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataframe.iloc[idx]\n",
    "        return {\n",
    "            'english': row['English words/sentences'],\n",
    "            'french': row['French words/sentences']\n",
    "        }\n",
    "\n",
    "# Build vocab\n",
    "source_vocab = build_vocab(df['English words/sentences'].tolist())\n",
    "target_vocab = build_vocab(df['French words/sentences'].tolist())\n",
    "\n",
    "# Make dataset and dataloader\n",
    "train_dataset = TranslationDataset(df_train)\n",
    "test_dataset = TranslationDataset(df_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, collate_fn=collate_fn, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, collate_fn=collate_fn, shuffle=True)\n",
    "\n",
    "# Check a batch\n",
    "for src_batch, tgt_batch in train_loader:\n",
    "    print(\"Source shape:\", src_batch.shape)\n",
    "    print(\"Target shape:\", tgt_batch.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb570e2c-3fdb-4884-80a1-9b9b05bc036a",
   "metadata": {},
   "source": [
    "## Model Architectuur\n",
    "We definiëren nu het Transformer-model. We maken gebruik van de encoder-decoder structuur met multi-head self-attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2add8a-4e6e-46ca-9d33-ea1ec8ddd814",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "from torch.nn import Transformer\n",
    "import math\n",
    "\n",
    "# Adds positional encoding to the token embedding\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, emb_size: int, dropout: float, maxlen: int = 100):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)\n",
    "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
    "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
    "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
    "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
    "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('pos_embedding', pos_embedding)\n",
    "\n",
    "    def forward(self, token_embedding: Tensor):\n",
    "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])\n",
    "\n",
    "# Tokens to embeddings\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size: int, emb_size):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
    "        self.emb_size = emb_size\n",
    "\n",
    "    def forward(self, tokens: Tensor):\n",
    "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)\n",
    "\n",
    "# Seq2Seq Network\n",
    "class Seq2SeqTransformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_encoder_layers: int,\n",
    "                 num_decoder_layers: int,\n",
    "                 emb_size: int,\n",
    "                 nhead: int,\n",
    "                 src_vocab_size: int,\n",
    "                 tgt_vocab_size: int,\n",
    "                 dim_feedforward: int = 512,\n",
    "                 dropout: float = 0.1):\n",
    "        super(Seq2SeqTransformer, self).__init__()\n",
    "        self.transformer = Transformer(d_model=emb_size,\n",
    "                                       nhead=nhead,\n",
    "                                       num_encoder_layers=num_encoder_layers,\n",
    "                                       num_decoder_layers=num_decoder_layers,\n",
    "                                       dim_feedforward=dim_feedforward,\n",
    "                                       dropout=dropout,\n",
    "                                       batch_first=False)\n",
    "        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n",
    "        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)\n",
    "        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)\n",
    "        self.positional_encoding = PositionalEncoding(emb_size, dropout=dropout)\n",
    "\n",
    "    def forward(self,\n",
    "                src: Tensor,\n",
    "                tgt: Tensor,\n",
    "                src_mask: Tensor,\n",
    "                tgt_mask: Tensor,\n",
    "                src_padding_mask: Tensor,\n",
    "                tgt_padding_mask: Tensor,\n",
    "                memory_key_padding_mask: Tensor):\n",
    "        src_emb = self.positional_encoding(self.src_tok_emb(src))\n",
    "        #print(src.shape, src_emb.shape, src_mask.shape)\n",
    "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(tgt))\n",
    "        #print(tgt.shape, tgt_emb.shape, tgt_mask.shape)\n",
    "        outs = self.transformer(src_emb, tgt_emb, src_mask, tgt_mask, None,\n",
    "                                src_padding_mask, tgt_padding_mask, memory_key_padding_mask)\n",
    "        return self.generator(outs)\n",
    "\n",
    "    def encode(self, src: Tensor, src_mask: Tensor):\n",
    "        return self.transformer.encoder(self.positional_encoding(\n",
    "                            self.src_tok_emb(src)), src_mask)\n",
    "\n",
    "    def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor):\n",
    "        return self.transformer.decoder(self.positional_encoding(\n",
    "                          self.tgt_tok_emb(tgt)), memory,\n",
    "                          tgt_mask)\n",
    "\n",
    "def generate_square_subsequent_mask(sz):\n",
    "    mask = (torch.triu(torch.ones((sz, sz), device=device)) == 1).transpose(0, 1)\n",
    "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    return mask\n",
    "\n",
    "def create_mask(src, tgt):\n",
    "    src_seq_len = src.shape[0]\n",
    "    tgt_seq_len = tgt.shape[0]\n",
    "\n",
    "    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n",
    "    src_mask = torch.zeros((src_seq_len, src_seq_len),device=device).type(torch.bool)\n",
    "\n",
    "    src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n",
    "    tgt_padding_mask = (tgt == PAD_IDX).transpose(0, 1)\n",
    "    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "SRC_VOCAB_SIZE = source_vocab_size\n",
    "TGT_VOCAB_SIZE = target_vocab_size\n",
    "EMB_SIZE = 24\n",
    "NHEAD = 2\n",
    "FFN_HID_DIM = 32\n",
    "NUM_ENCODER_LAYERS = 1\n",
    "NUM_DECODER_LAYERS = 1\n",
    "\n",
    "# maak model\n",
    "transformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE,\n",
    "                                 NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM)\n",
    "# initialiseer gewichten\n",
    "for p in transformer.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "\n",
    "transformer = transformer.to(device)\n",
    "print(transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8eed5d-3180-4557-9d91-50e3eed1d4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (src, tgt) in enumerate(train_loader):\n",
    "    print(\"Batch shapes:\", src.shape, tgt.shape)  # (batch, seq_len)\n",
    "\n",
    "    src = src.to(device)\n",
    "    tgt = tgt.to(device)\n",
    "\n",
    "    # Use target input shifted by one (teacher forcing)\n",
    "    tgt_input = tgt[:-1, ]   # everything except last token\n",
    "    tgt_out   = tgt[1:, 1:]    # expected output (shifted by one)\n",
    "\n",
    "    # Create masks\n",
    "    src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
    "\n",
    "    with torch.no_grad():  # no gradients, inference mode\n",
    "        logits = transformer(\n",
    "            src, tgt_input, \n",
    "            src_mask, tgt_mask,\n",
    "            src_padding_mask, tgt_padding_mask, src_padding_mask\n",
    "        )\n",
    "\n",
    "    print(\"Logits shape:\", logits.shape)  \n",
    "    # → (batch_size, tgt_seq_len, vocab_size)\n",
    "\n",
    "    # Convert logits to predicted token IDs\n",
    "    preds = torch.argmax(logits, dim=-1)\n",
    "    print(\"Predictions shape:\", preds.shape)  \n",
    "    # → (batch_size, tgt_seq_len)\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5788999-55c9-4728-842d-8951ae4074f4",
   "metadata": {},
   "source": [
    "## Training en Evaluatie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7ed06e-b519-4c79-855e-aa0391d9d1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "optimizer = torch.optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "def compute_loss(model, loss_fn, src, tgt):\n",
    "    src = src.to(device)\n",
    "    tgt = tgt.to(device)\n",
    "\n",
    "    tgt_input = tgt[:-1, :]\n",
    "\n",
    "    src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
    "    logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
    "\n",
    "    tgt_out = tgt[1:, :]\n",
    "    return loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
    "\n",
    "def train_epoch(model, optimizer, dataloader):\n",
    "    model.train()\n",
    "    losses = 0\n",
    "    total_batches = len(dataloader)\n",
    "    progress_interval = total_batches // 10\n",
    "    \n",
    "    for i, (src, tgt) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        loss = compute_loss(model, loss_fn, src, tgt)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses += loss.item()\n",
    "        if (i + 1) % progress_interval == 0:\n",
    "            progress_percentage = ((i + 1) / total_batches) * 100\n",
    "            print(f\"Training progress: {progress_percentage:.1f}% - Batch {i + 1}/{total_batches}\")\n",
    "    return losses / len(list(dataloader))\n",
    "\n",
    "NUM_EPOCHS = 10\n",
    "\n",
    "# Function to save the model\n",
    "def save_model(model, optimizer, model_path):\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "    }, model_path)\n",
    "    print(f\"Model saved to {model_path}\")\n",
    "    \n",
    "# Function to load the model\n",
    "def load_model(model, optimizer, model_path):\n",
    "    if os.path.exists(model_path):\n",
    "        checkpoint = torch.load(model_path)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        print(f\"Model loaded from {model_path}\")\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "# Check if the model exists, if so load it, otherwise train and save\n",
    "if not load_model(transformer, optimizer, \"transformer_model.pth\"):\n",
    "    for epoch in range(1, NUM_EPOCHS+1):\n",
    "        start_time = timer()\n",
    "        train_loss = train_epoch(transformer, optimizer, train_loader)\n",
    "        end_time = timer()\n",
    "        print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Epoch time = {(end_time - start_time):.3f}s\"))\n",
    "    save_model(transformer, optimizer, \"transformer_model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f079b704-ecf3-43a4-a0ba-47aff6a5722f",
   "metadata": {},
   "source": [
    "## Vertalingen: OEFENING\n",
    "\n",
    "Ga met je favoriete AI-tool op zoek naar een implementatie voor de translate_sentence functie.\n",
    "Deze functie moet woord per woord een zin genereren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c101f7-35b7-4818-96bf-800883b0cde5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_key_from_value(my_dict, target_value):\n",
    "    for key, value in my_dict.items():\n",
    "        if value == target_value:\n",
    "            return key\n",
    "    return None\n",
    "\n",
    "def translate_sentence(model, src_sentence, src_vocab, tgt_vocab, max_len=50):\n",
    "    model.eval()\n",
    "    \n",
    "    src_tokens = [src_vocab.get(token, UNK_IDX) for token in src_sentence.split()]\n",
    "    src_tensor = torch.tensor(src_tokens).unsqueeze(1).to(device)\n",
    "    src_mask = torch.zeros((src_tensor.shape[0], src_tensor.shape[0]), device=device).type(torch.bool)\n",
    "    \n",
    "    memory = model.encode(src_tensor, src_mask)\n",
    "    \n",
    "    tgt_tokens = [BOS_IDX]\n",
    "    tgt_tensor = torch.tensor(tgt_tokens, dtype=torch.long).unsqueeze(1).to(device)\n",
    "    for _ in range(max_len):\n",
    "        tgt_mask = generate_square_subsequent_mask(tgt_tensor.size(0)).to(device)\n",
    "        output = model.decode(tgt_tensor, memory, tgt_mask)\n",
    "        output_logits = model.generator(output[-1, :])\n",
    "        next_token = torch.argmax(output_logits, dim=-1).item()\n",
    "        tgt_tokens.append(next_token)\n",
    "        tgt_tensor = torch.tensor(tgt_tokens, dtype=torch.long).unsqueeze(1).to(device)\n",
    "        if next_token == EOS_IDX:\n",
    "            break\n",
    "    translated_tokens = [get_key_from_value(tgt_vocab, tok) for tok in tgt_tokens[1:-1]]\n",
    "    translated_sentence = \" \".join(translated_tokens)\n",
    "    return translated_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24dee002-57aa-4d96-af0d-326c99ace442",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Usage\n",
    "src_sentence = \"translate this text to French\"\n",
    "translated_sentence = translate_sentence(transformer, src_sentence, source_vocab, target_vocab)\n",
    "print(f\"Translated sentence: {translated_sentence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f289c8c0-c5b9-45cb-9a29-cc3c8421ee48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24bb64fc-d2bb-48d7-8324-67a06c1a4c82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73aa8a4-051c-4df5-951b-e718c0584428",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
