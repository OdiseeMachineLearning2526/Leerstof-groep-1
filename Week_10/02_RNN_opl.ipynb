{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ece9ec2a-8afa-4f7f-bfe9-0609d1100f6b",
   "metadata": {},
   "source": [
    "# Classificatie van nieuwsartikelen\n",
    "\n",
    "In deze notebook gaan we verder werken op de AG-news nieuwsartikelen dataset.\n",
    "In de vorige notebook hebben we bekeken hoe we tekstuele data kunnen preprocessen.\n",
    "In deze notebook gaan we classificatie uitvoeren door gebruik te maken van recurrente neurale netwerken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7c6d37-e785-4177-9e0d-8cebff1cbd4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.12), please consider upgrading to the latest version (0.3.13).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>Wall St. Bears Claw Back Into the Black (Reuters)</td>\n",
       "      <td>Reuters - Short-sellers, Wall Street's dwindli...</td>\n",
       "      <td>Wall St. Bears Claw Back Into the Black (Reute...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Carlyle Looks Toward Commercial Aerospace (Reu...</td>\n",
       "      <td>Reuters - Private investment firm Carlyle Grou...</td>\n",
       "      <td>Carlyle Looks Toward Commercial Aerospace (Reu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Oil and Economy Cloud Stocks' Outlook (Reuters)</td>\n",
       "      <td>Reuters - Soaring crude prices plus worries\\ab...</td>\n",
       "      <td>Oil and Economy Cloud Stocks' Outlook (Reuters...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>Iraq Halts Oil Exports from Main Southern Pipe...</td>\n",
       "      <td>Reuters - Authorities have halted oil export\\f...</td>\n",
       "      <td>Iraq Halts Oil Exports from Main Southern Pipe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>Oil prices soar to all-time record, posing new...</td>\n",
       "      <td>AFP - Tearaway world oil prices, toppling reco...</td>\n",
       "      <td>Oil prices soar to all-time record, posing new...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                              title  \\\n",
       "0      2  Wall St. Bears Claw Back Into the Black (Reuters)   \n",
       "1      2  Carlyle Looks Toward Commercial Aerospace (Reu...   \n",
       "2      2    Oil and Economy Cloud Stocks' Outlook (Reuters)   \n",
       "3      2  Iraq Halts Oil Exports from Main Southern Pipe...   \n",
       "4      2  Oil prices soar to all-time record, posing new...   \n",
       "\n",
       "                                         description  \\\n",
       "0  Reuters - Short-sellers, Wall Street's dwindli...   \n",
       "1  Reuters - Private investment firm Carlyle Grou...   \n",
       "2  Reuters - Soaring crude prices plus worries\\ab...   \n",
       "3  Reuters - Authorities have halted oil export\\f...   \n",
       "4  AFP - Tearaway world oil prices, toppling reco...   \n",
       "\n",
       "                                                text  \n",
       "0  Wall St. Bears Claw Back Into the Black (Reute...  \n",
       "1  Carlyle Looks Toward Commercial Aerospace (Reu...  \n",
       "2  Oil and Economy Cloud Stocks' Outlook (Reuters...  \n",
       "3  Iraq Halts Oil Exports from Main Southern Pipe...  \n",
       "4  Oil prices soar to all-time record, posing new...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of vocab: 20002\n",
      "X_train shape: torch.Size([30000, 100])\n",
      "Highest token idx train tensor(20001)\n",
      "Highest token idx test tensor(19998)\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import re\n",
    "import kagglehub\n",
    "\n",
    "# Parameters\n",
    "MAX_NUM_WORDS = 20000  # Maximum number of unique words to keep\n",
    "MAX_SEQUENCE_LENGTH = 100  # Maximum length of input sequences\n",
    "EMBEDDING_DIM = 50  # Dimensionality of the embedding layer\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device='cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# get the dataset\n",
    "path = kagglehub.dataset_download(\"amananandrai/ag-news-classification-dataset\")\n",
    "\n",
    "# Load the dataset\n",
    "def read_csv(filename):\n",
    "    df = pd.read_csv(filename)\n",
    "    df.columns = [\"label\", \"title\", \"description\"]\n",
    "    df[\"text\"] = df['title'] + ' ' + df['description']\n",
    "    df['label'] = df['label'] - 1\n",
    "    return df\n",
    "\n",
    "df_train = read_csv(f'{path}/train.csv')\n",
    "df_test = read_csv(f'{path}/test.csv')\n",
    "display(df_train.head())\n",
    "\n",
    "# tokenizer\n",
    "def simple_tokenizer(text):\n",
    "    return re.findall(r\"\\b\\w+\\b\", text.lower())\n",
    "    \n",
    "# Build vocab from training set\n",
    "counter = Counter()\n",
    "for text in df_train[\"text\"]:\n",
    "    counter.update(simple_tokenizer(text)) # splits de tekst kolom in woorden, tel hoeveel keer elk woord voorkomt\n",
    "\n",
    "vocab = {word: idx+2 for idx, (word, _) in enumerate(counter.most_common(MAX_NUM_WORDS))}\n",
    "vocab[\"<PAD>\"] = 0 # special token for padding\n",
    "vocab[\"<UNK>\"] = 1 # special token for unknown words\n",
    "\n",
    "print(\"Size of vocab:\", len(vocab))\n",
    "\n",
    "def encode(text, vocab, max_len=MAX_SEQUENCE_LENGTH):\n",
    "    tokens = simple_tokenizer(text) # splits text in woorden\n",
    "    ids = [vocab.get(tok, vocab[\"<UNK>\"]) for tok in tokens] # zet elk woord om naar een token (unknown als het niet bestaat)\n",
    "    ids = ids[:max_len] # truncate de text\n",
    "    ids += [vocab[\"<PAD>\"]] * (max_len - len(ids)) # voeg padding toe als de tekst te kort is\n",
    "    return ids\n",
    "\n",
    "num_samples = 30000\n",
    "X_train = [encode(text, vocab) for text in df_train[\"description\"][:num_samples]]  # neem subset voor demo\n",
    "y_train = df_train[\"label\"][:num_samples]\n",
    "X_train = torch.tensor(X_train)\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print('Highest token idx train', torch.max(X_train))\n",
    "\n",
    "X_test = [encode(text, vocab) for text in df_test[\"description\"][:num_samples]]  # neem subset voor demo\n",
    "X_test = torch.tensor(X_test)\n",
    "y_test = df_test[\"label\"][:num_samples]\n",
    "print('Highest token idx test', torch.max(X_test))\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.texts[idx], dtype=torch.long), torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "\n",
    "train_dataset = TextDataset(X_train, y_train)\n",
    "test_dataset = TextDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fdd6c70-ace6-4e59-8973-b6a60f61fadf",
   "metadata": {},
   "source": [
    "## Opbouwen, trainen en evalueren van een RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "08f036a6-5932-4948-aa2f-ce4129738238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNNModel(\n",
      "  (embedding): Embedding(20002, 50)\n",
      "  (rnn): RNN(50, 128, batch_first=True)\n",
      "  (fc): Linear(in_features=128, out_features=4, bias=True)\n",
      ")\n",
      "torch.Size([64, 100]) torch.Size([64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jens.baetens3\\AppData\\Local\\Temp\\ipykernel_17308\\2886902385.py:78: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(self.texts[idx], dtype=torch.long), torch.tensor(self.labels[idx], dtype=torch.long)\n"
     ]
    }
   ],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #print(x.shape, torch.max(x))\n",
    "        embedded = self.embedding(x)\n",
    "        output, hidden = self.rnn(embedded) # 1 laag met rnn-cellen (woord per woord wordt hierdoor gestuurd)\n",
    "        last_output = hidden[-1]\n",
    "        out = self.fc(last_output)\n",
    "        return out\n",
    "\n",
    "# Parameters\n",
    "hidden_dim = 128\n",
    "output_dim = 4  # AG-News heeft 4 klassen\n",
    "\n",
    "model = RNNModel(MAX_NUM_WORDS+2, EMBEDDING_DIM, hidden_dim, output_dim)\n",
    "print(model)\n",
    "\n",
    "texts, labels = next(iter(train_loader))\n",
    "print(texts.shape, labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5bf02096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jens.baetens3\\AppData\\Local\\Temp\\ipykernel_17308\\2886902385.py:78: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(self.texts[idx], dtype=torch.long), torch.tensor(self.labels[idx], dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2], Loss: 1.3925\n",
      "Epoch [2/2], Loss: 1.3940\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Definieer de Loss en Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.002)\n",
    "\n",
    "# Train het Model\n",
    "num_epochs = 2\n",
    "model.train()\n",
    "print(\"Training started.\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        #inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}\")\n",
    "\n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9eb57af5-0ff0-499f-94be-35e7e5ea89e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jens.baetens3\\AppData\\Local\\Temp\\ipykernel_17308\\2886902385.py:78: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(self.texts[idx], dtype=torch.long), torch.tensor(self.labels[idx], dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 24.46%\n"
     ]
    }
   ],
   "source": [
    "# Evalueer het Model\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        predicted = torch.argmax(outputs, 1)\n",
    "        \n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f\"Test Accuracy: {100 * correct / total:.2f}%\")\n",
    "\n",
    "# Opslaan van het Model\n",
    "torch.save(model.state_dict(), 'rnn_ag_news_model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6277e4fc-e559-4f43-8d88-1840cc31dc52",
   "metadata": {},
   "source": [
    "## Oefeningen\n",
    "\n",
    "* Voeg een extra Dense-laag toe na de RNN-laag. Experimenteer met het aantal neuronen in deze laag en analyseer hoe de prestaties veranderen.\n",
    "* Pas het model aan om in plaats van een basis RNN-laag een LSTM of GRU-laag te gebruiken. Vergelijk de prestaties van de drie typen recurrente netwerken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8991aec0-9549-4d1b-99fb-897d2331fb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oefening 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7775c80b-0838-4a41-848a-25c499141be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oefening 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0a7182-6e98-40a4-953d-3051cb8687a9",
   "metadata": {},
   "source": [
    "**Oefening 3**\n",
    "\n",
    "Volg de tutorial op de volgende link: https://www.tensorflow.org/text/tutorials/text_generation\n",
    "Werk hieronder het gelijkaardige probleem uit maar maak het door gebruik te maken van pytorch in plaats van tensorflow voor het model op te bouwen.\n",
    "In deze tutorial wordt er tekst gegenereerd die lijkt op tekst geschreven door shakespeare.\n",
    "Let op dat dit een vereenvoudigde versie is waarbij karakter per karakter wordt gegenereerd en niet woord per woord. Er is dus geen garantie dat er echte woorden gemaakt worden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edab02f2-fa9f-4306-8ad2-a378b45eb9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras_core as keras\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "import random\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "path_to_file = keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
    "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
    "print(f'Length of text: {len(text)} characters')\n",
    "print(text[:250])\n",
    "# The unique characters in the file\n",
    "vocab = sorted(set(text))\n",
    "print(f'{len(vocab)} unique characters')\n",
    "\n",
    "# Character to index mapping\n",
    "char_to_idx = {char: idx for idx, char in enumerate(vocab)}\n",
    "idx_to_char = {idx: char for idx, char in enumerate(vocab)}\n",
    "\n",
    "# Encode the text as integers\n",
    "encoded_text = [char_to_idx[char] for char in text]\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, encoded_text, seq_length):\n",
    "        self.encoded_text = encoded_text\n",
    "        self.seq_length = seq_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        # Total sequences in the dataset\n",
    "        return len(self.encoded_text) - self.seq_length-1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Input sequence\n",
    "        x = self.encoded_text[idx:idx + self.seq_length]\n",
    "        # Target (next character)\n",
    "        y = self.encoded_text[idx + 1: idx + self.seq_length+1]\n",
    "        return torch.tensor(x, dtype=torch.long), torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "# Example usage\n",
    "seq_length = 100\n",
    "dataset = TextDataset(encoded_text, seq_length)\n",
    "\n",
    "\n",
    "subset_size = int(0.1 * len(dataset))\n",
    "random_indices = random.sample(range(len(dataset)), subset_size)\n",
    "dataset = Subset(dataset, random_indices)\n",
    "\n",
    "# Check a single example\n",
    "sample_x, sample_y = dataset[0]\n",
    "print(\"Input (x):\", sample_x)\n",
    "print(\"Target (y):\", sample_y)\n",
    "print(\"Decoded Input:\", ''.join(idx_to_char[idx] for idx in sample_x.numpy()))\n",
    "print(\"Decoded Target:\", ''.join([idx_to_char[x] for x in sample_x.numpy()]))\n",
    "print('Rows', len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910b060e-a61f-4a32-9dcd-ba625ebc222c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(idx_to_char)\n",
    "embedding_dim = 50\n",
    "rnn_units = 150\n",
    "\n",
    "class ShakespeareModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
    "        super(ShakespeareModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = nn.GRU(embedding_dim, rnn_units, batch_first=True)\n",
    "        self.output = nn.Linear(rnn_units, vocab_size)\n",
    "\n",
    "    def forward(self, inputs, states=None, return_state=False):\n",
    "        # Embedding layer\n",
    "        x = self.embedding(inputs)\n",
    "        \n",
    "        # GRU layer\n",
    "        if states is None:\n",
    "            x, states = self.gru(x)\n",
    "        else:\n",
    "            x, states = self.gru(x, states)\n",
    "        \n",
    "        # Dense layer\n",
    "        x = self.output(x)\n",
    "        \n",
    "        return (x, states[:, -1:, :]) if return_state else x\n",
    "\n",
    "shakespeare = ShakespeareModel(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd60acb-29d3-4087-b7a9-8f64b2c9fd64",
   "metadata": {},
   "outputs": [],
   "source": [
    "for input_example_batch, target_example_batch in dataset:\n",
    "    print(input_example_batch.shape)\n",
    "    example_batch_predictions, states = shakespeare(input_example_batch.unsqueeze(0), return_state=True)\n",
    "    print(example_batch_predictions.shape, \"# (sequence_length, vocab_size), 100 char as input\")\n",
    "    print(states.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45fbb345-d9f0-477a-98c2-2aa83db41963",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import math\n",
    "\n",
    "batch_size = 64\n",
    "seq_length = 100\n",
    "epochs = 5\n",
    "\n",
    "\n",
    "# Create the dataset and DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(shakespeare.parameters(), lr=0.005)\n",
    "\n",
    "# Set the path to save/load the model\n",
    "model_path = \"rnns.pth\"\n",
    "\n",
    "# Check if the model file exists\n",
    "if os.path.exists(model_path):\n",
    "    print(\"Loading existing model...\")\n",
    "    shakespeare.load_state_dict(torch.load(model_path))\n",
    "    shakespeare.eval()  # Set model to evaluation mode if only inference is required\n",
    "    shakespeare = shakespeare.to(device)\n",
    "    print(\"Model loaded...\")\n",
    "else:\n",
    "    shakespeare = ShakespeareModel(\n",
    "        vocab_size=vocab_size,\n",
    "        embedding_dim=embedding_dim,\n",
    "        rnn_units=rnn_units).to(device)\n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        shakespeare.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for batch, (inputs, targets) in enumerate(dataloader):\n",
    "            # Move data to the appropriate device (CPU/GPU)\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            \n",
    "            # Zero the gradient\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = shakespeare(inputs)  # Shape: (batch_size, seq_length, vocab_size)\n",
    "            # Reshape outputs and targets for loss computation\n",
    "            outputs = outputs.view(-1, vocab_size)  # Shape: (batch_size * seq_length, vocab_size)\n",
    "            targets = targets.view(-1)  # Shape: (batch_size * seq_length)\n",
    "             \n",
    "            # Compute the loss\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "\n",
    "            if batch % int((len(dataloader)/10)) == 0:\n",
    "                print(f\"Epoch {epoch + 1}/{epochs}: {math.floor(batch/len(dataloader)*100)}\")\n",
    "        \n",
    "        # Print epoch loss\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    # Save the model after training\n",
    "    torch.save(shakespeare.state_dict(), model_path)\n",
    "    print(f\"Model saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec92677b-7f6b-4cb1-9f6b-faf2da8a68b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_string, char_to_idx, idx_to_char, vocab_size, generation_length=100, temperature=1.0):\n",
    "    model.eval()\n",
    "    input_indices = torch.tensor([char_to_idx[char] for char in start_string], dtype=torch.long).unsqueeze(0).to(device)\n",
    "\n",
    "    generated_text = start_string\n",
    "    \n",
    "    # Prime hidden state with the start string\n",
    "    with torch.no_grad():\n",
    "        outputs, states = model(input_indices, states=None, return_state=True)\n",
    "\n",
    "    # Start with the last character\n",
    "    input_indices = input_indices[:, -1:]\n",
    "    print(states.shape)\n",
    "\n",
    "    for _ in range(generation_length):\n",
    "        with torch.no_grad():\n",
    "            outputs, states = model(input_indices, states=states, return_state=True)\n",
    "\n",
    "        # take only the last timestep\n",
    "        logits = outputs[:, -1, :]  # (1, vocab_size)\n",
    "\n",
    "        # om te bepalen hoe random je model is (moet je niet kennen voor theorie/praktijk)\n",
    "        logits = logits / temperature\n",
    "        probabilities = F.softmax(logits, dim=-1)\n",
    "\n",
    "        next_index = torch.multinomial(probabilities.squeeze(), num_samples=1)\n",
    "        generated_text += idx_to_char[next_index.item()]\n",
    "\n",
    "        # feed next char back in\n",
    "        input_indices = next_index.unsqueeze(0).to(device)\n",
    "\n",
    "    return generated_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d49248-9fad-4174-a5a1-af319bcfc38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example start string and generation parameters\n",
    "start_string = \"ROMEO: \"\n",
    "generation_length = 200\n",
    "temperature = 0.8\n",
    "\n",
    "# Generate text\n",
    "generated_text = generate_text(\n",
    "    model=shakespeare,\n",
    "    start_string=start_string,\n",
    "    char_to_idx=char_to_idx,\n",
    "    idx_to_char=idx_to_char,\n",
    "    vocab_size=vocab_size,\n",
    "    generation_length=generation_length,\n",
    "    temperature=temperature\n",
    ")\n",
    "\n",
    "print(\"Generated Text:\")\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645098d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5489145",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
