{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10999264",
   "metadata": {},
   "source": [
    "# Introductie tot Tensors en Neurale Netwerken\n",
    "In deze oefening maak je kennis met het gebruik van **tensors** en het bouwen van een eenvoudig neuraal netwerk met Ã©Ã©n verborgen laag. We zullen stap voor stap de **forward pass** uitvoeren, en daarna ook een voorbeeld van de **backward pass** (backpropagation) bekijken.\n",
    "\n",
    "We gebruiken hierbij [PyTorch](https://pytorch.org/docs/stable/index.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb3b567",
   "metadata": {},
   "source": [
    "## Stap 1: Importeren van PyTorch\n",
    "We beginnen met het importeren van PyTorch en het aanmaken van tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4317db30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ff4d2d3e",
   "metadata": {},
   "source": [
    "## Stap 2: Voorbeeld data en gewichten\n",
    "We maken voorbeeld data (X, y) en initialiseren gewichten en biases voor een netwerk met Ã©Ã©n verborgen laag.\n",
    "De input stelt een dataset voor van 2 rijen met drie features. Maak hier dus een tensor voor van dimensie 2 rijen en 3 kolommen. \n",
    "De output/labels (y) is een getal dat we proberen te voorspellen. Dit bevat 1 waarde per rij en is dus een tensor met dimensie 2 rijen en 1 kolom.\n",
    "De waarden in deze tensors mag je zelf kiezen.\n",
    "\n",
    "Daarna maak je ook de nodige tensors aan voor het neuraal netwerk.\n",
    "We gaan werken met 1 hidden layer van 4 neuronen). Dit houdt dus in dat er twee berekeningen stappen zijn (input -> hidden en hidden -> output).\n",
    "Elk stap heeft twee tensors nodig (een gewichten tensor en een bias).\n",
    "Zorg dus voor de volgende tensors met random waarden:\n",
    "\n",
    "* w1 van dimensie 3 bij 4 (3 kolommen van X naar 4 neuronen)\n",
    "* b1 van dimensie 4 bij 1 (1 waarde per neuron)\n",
    "* w2 van dimensie 4 bij 1 (4 neuronen naar 1 output per rij)\n",
    "* b2 van dimensie 1 bij 1 (1 waarde per output)\n",
    "\n",
    "Print de tensors en hun shape uit om je initialisatie te testen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ccedf6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b046a74b",
   "metadata": {},
   "source": [
    "## Stap 3: Forward pass\n",
    "\n",
    "In de volgende stap wordt de output berekend op basis van de output. Dit wordt ook het maken van een voorspelling genoemd.\n",
    "In het netwerk met 1 hidden laag moeten we de volgende stappen uitvoeren.\n",
    "\n",
    "1. Bereken de activatie van de verborgen laag door de volgende berekening te doen X * w1 + b1 (let op dat dit matrices zijn dus bekijk onderstaande tip voor de nodige berekening uit te voeren).\n",
    "2. Pas een **ReLU** activatiefunctie toe\n",
    "3. Bereken de output van het netwerk (zelfde berekening als stap 1)\n",
    "\n",
    "Zie ook: [torch.matmul](https://pytorch.org/docs/stable/generated/torch.matmul.html), [torch.nn.functional.relu](https://pytorch.org/docs/stable/generated/torch.nn.functional.relu.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989a41cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "00d59891",
   "metadata": {},
   "source": [
    "## Stap 4: Loss functie\n",
    "\n",
    "Bovenstaande berekening gebeurde met random waarden voor de matrices w1, b1, w2 en b2.\n",
    "Het is dus te verwachten dat dit totaal geen goed resultaat zal geven.\n",
    "Om te berekenen hoe mis het resultaat is wordt bij een regressieprobleem (wat we hier oplossen) vaak de mean squared error loss-functie gebruikt.\n",
    "Bereken nu deze loss-waarde.\n",
    "\n",
    "Zie ook: [torch.nn.functional.mse_loss](https://docs.pytorch.org/docs/stable/generated/torch.nn.functional.mse_loss.html#torch.nn.functional.mse_loss)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29f5d5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "371975b9",
   "metadata": {},
   "source": [
    "## Stap 5: Backward pass\n",
    "\n",
    "Met de berekende fout of loss-waarde kunnen we nu de getallen in de w1, b1, w2 en b2 optimaliseren zodat de berekening beter werkt.\n",
    "Dit proces noemt backpropagation (de fout van de output naar de input laten gaan en ondertussen de gewichten updaten).\n",
    "\n",
    "Voer nu backpropagation uit om de gradiÃ«nten van de parameters te berekenen.\n",
    "De gradienten zijn de afgeleiden of de waarde waarmee de parameters moeten aangepast worden.\n",
    "\n",
    "Zie ook: [torch.Tensor.backward](https://pytorch.org/docs/stable/generated/torch.Tensor.backward.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009ad613",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "96d5ab30-02c6-4651-bf75-b6bff926413f",
   "metadata": {},
   "source": [
    "# Stap 6: Update parameters met gradient descent\n",
    "\n",
    "De bovenstaande berekende waarden kunnen gebruikt worden om de parameters aan te passen.\n",
    "In de praktijk wordt er gebruik gemaakt van een learning rate zodat de gradient niet volledig wordt toegevoegd maar maar voor een bepaald percentage. Anders ga je een onstabiel leerproces bekomen omdat de gewichten te snel veranderen.\n",
    "Dit om voor een stabieler leerproces te zorgen zodat het niet overreageert op de laatst geziene batch. Dit doen we hier echter niet.\n",
    "Gebruik hieronder een learning rate van 0.1.\n",
    "\n",
    "Doe dus voor de 4 matrices w1, b1, w2 en b2 de volgende berekening:\n",
    "\n",
    "```\n",
    "    w1 = w1 - learning_rw1.grad\n",
    "```\n",
    "\n",
    "Zet tenslotte ook de berekende gradient terug op 0 zodat het terug klaar zat voor een volgende leerstap. Bekijk hiervoor de [zero_](https://docs.pytorch.org/docs/stable/generated/torch.Tensor.zero_.html#torch.Tensor.zero_) functie.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9300abf8-a561-4e17-b85b-68d16d3204cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3f307d2b-91dd-4713-9f72-a086b43fbda6",
   "metadata": {},
   "source": [
    "# Stap 7: Opnieuw berekenen van output en loss na de update\n",
    "\n",
    "Bereken nu opnieuw de output en de loss na het updaten van de gewichten.\n",
    "Werkt het leerproces?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf757b2-73e5-4b3b-b1a4-664f415a39b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5b7ed228",
   "metadata": {},
   "source": [
    "## Conclusie\n",
    "- Je hebt een forward pass uitgevoerd door middel van matrixvermenigvuldiging en activatiefuncties.\n",
    "- Je hebt een loss berekend met Binary Cross Entropy.\n",
    "- Je hebt een backward pass uitgevoerd om de gradiÃ«nten van de parameters te verkrijgen.\n",
    "\n",
    "Dit vormt de basis van hoe neurale netwerken leren! ðŸš€"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
