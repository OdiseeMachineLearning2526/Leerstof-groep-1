{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73e730cc-69c0-4b82-bb58-25a163fa4aad",
   "metadata": {},
   "source": [
    "# Variational Autoencoder met Fashion MNIST\n",
    "\n",
    "Een Variational Autoencoder (VAE) is een type autoencoder dat wordt gebruikt voor het genereren van nieuwe voorbeelden die vergelijkbaar zijn met de dataset waarop het is getraind. \n",
    "Het bestaat uit een encoder die de input data naar een lagere-dimensionale latente ruimte projecteert, en een decoder die uit deze latente ruimte nieuwe data reconstrueert.\n",
    "\n",
    "Overzicht:\n",
    "* Importeren van bibliotheken en dataset\n",
    "* Definiëren van de VAE-architectuur\n",
    "* Trainen van het VAE-model\n",
    "* Genereren van nieuwe afbeeldingen met de VAE\n",
    "## Importeren van packages en dataset\n",
    "\n",
    "Eerst importeren we alle benodigde Python-bibliotheken voor het bouwen, trainen en visualiseren van onze VAE.\n",
    "We gebruiken Pytorch voor het bouwen van het neurale netwerk, matplotlib voor visualisaties en NumPy voor numerieke berekeningen.\n",
    "Daarna laden we de Fashion MNIST dataset, normaliseren de pixelwaarden naar de range [0,1] \n",
    "en splitsen de dataset in een trainings- en testset. We gebruiken DataLoader om mini-batches te maken voor training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1737c9cf-73df-433c-95b1-aa0c9e36f383",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Check of GPU beschikbaar is\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Apparaat gebruikt voor training: {device}\")\n",
    "\n",
    "# Definieer transformatie (normaliseren van beelden)\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Converteer afbeeldingen naar PyTorch tensors\n",
    "])\n",
    "\n",
    "# Laad de Fashion MNIST dataset\n",
    "train_dataset = datasets.FashionMNIST(root='./data', train=True, transform=transform, download=True)\n",
    "test_dataset = datasets.FashionMNIST(root='./data', train=False, transform=transform, download=True)\n",
    "\n",
    "# Creëer DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "print(f\"Aantal trainingsbatches: {len(train_loader)}, Aantal testbatches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b62ec61-8ac0-42a1-ba86-e73bafdc6503",
   "metadata": {},
   "source": [
    "## Definiëren van de VAE-architectuur\n",
    "\n",
    "De architectuur van onze Variational Autoencoder bestaat uit een encoder die de inputbeelden naar een lagere-dimensionale latente ruimte projecteert en een decoder die deze latente ruimte gebruikt om de afbeeldingen opnieuw te genereren.\n",
    "De verliesfunctie voor onze VAE, bestaande uit de som van de reconstructieverlies (binary cross-entropy)\n",
    "en de KL-divergentie. We gebruiken de Adam optimizer voor het bijwerken van de gewichten van het netwerk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3be7c3-68ec-4c0b-8491-959e28e5138f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dit gaat niet letterlijk als oefening gegeven worden in het praktijk gedeelte\n",
    "class VariationalAutoencoder(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(VariationalAutoencoder, self).__init__()\n",
    "        \n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, stride=2, padding=1), # aantal in kanalen is 1 want grijswaardenbeelden\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1), # 64 * 7 * 7\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Flatten()\n",
    "        )\n",
    "\n",
    "        # bottleneck\n",
    "        # mean\n",
    "        self.fc_mu  = nn.Linear(64*7*7, latent_dim)\n",
    "        # std\n",
    "        self.fc_logvar = nn.Linear(64*7*7, latent_dim)\n",
    "\n",
    "        self.decoder_input = nn.Linear(latent_dim, 64*7*7)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 1, kernel_size=3, stride=2, padding= 1, output_padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def encode(self, x):\n",
    "        x = self.encoder(x)\n",
    "        return self.fc_mu(x), self.fc_logvar(x)\n",
    "\n",
    "    def decode(self, x):\n",
    "        x = self.decoder_input(x)\n",
    "        # resize van 64*7*7 -> (64, 7, 7)\n",
    "        x = x.view(-1, 64, 7, 7)\n",
    "        return self.decoder(x)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + std * eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93cfa01d-f667-4951-9900-1847d914e32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 2\n",
    "model = VariationalAutoencoder(latent_dim).to(device)\n",
    "print(model)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.001)\n",
    "# functie moet je niet zelf kunnen opstellen\n",
    "def vae_loss(recon_x, x, mu, logvar):\n",
    "    recon_loss = nn.functional.binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "\n",
    "    kl_loss = -0.5 * torch.sum(1+logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "    return recon_loss + kl_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6e15e2-f0ab-4cb9-9131-9cdb5295c785",
   "metadata": {},
   "source": [
    "## Trainen van het VAE model\n",
    "\n",
    "In deze cel trainen we het VAE-model met de trainingsgegevens. \n",
    "Voor elke epoch voeren we een forward pass, berekenen we het verlies, en voeren we een backward pass uit om de gewichten bij te werken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7b39ed-f6fd-41de-b26b-8b76eba1b845",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0\n",
    "    for images, _ in train_loader:\n",
    "        images = images.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        images2, mu, logvar = model(images)\n",
    "        loss = vae_loss(images2, images, mu, logvar)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch}/{num_epochs}, loss: {running_loss/len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a911668-67ab-4f76-bd89-99f318b6fb6d",
   "metadata": {},
   "source": [
    "## Genereren van nieuwe resultaten\n",
    "\n",
    "Eerst bestuderen we de latente ruimte door de testset in kaart te brengen in de 2D latente ruimte van de VAE. Hiermee kunnen we zien hoe goed de VAE leert om vergelijkbare afbeeldingen bij elkaar te plaatsen.\n",
    "Daarna genereren we nieuwe afbeeldingen door willekeurige punten te nemen uit de latente ruimte \n",
    "en deze te decoderen met behulp van de decoder van de VAE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf185db7-75c2-440f-87d8-36de4adaad4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Genereer nieuwe afbeeldingen van random punten in de latente ruimte\n",
    "n = 10  # Aantal afbeeldingen per rij/kolom\n",
    "figure = np.zeros((28 * n, 28 * n))\n",
    "\n",
    "# Uniform verdeeld random punten in de latente ruimte\n",
    "grid_x = np.linspace(-3, 3, n)\n",
    "grid_y = np.linspace(-3, 3, n)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i, yi in enumerate(grid_y):\n",
    "        for j, xi in enumerate(grid_x):\n",
    "            z_sample = torch.tensor([[xi, yi]], dtype=torch.float32).to(device)\n",
    "            x_decoded = model.decode(z_sample).cpu().numpy()\n",
    "            digit = x_decoded[0].reshape(28, 28)\n",
    "            figure[i * 28: (i + 1) * 28, j * 28: (j + 1) * 28] = digit\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(figure, cmap='Greys_r')\n",
    "plt.title(\"Gegenereerde afbeeldingen van random punten in de latente ruimte\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d189d0db-f274-4810-abb3-5f3d5aa091ed",
   "metadata": {},
   "source": [
    "# Oefening: denoising autoencoder\n",
    "\n",
    "Een denoising autoencoder (DAE) is een type autoencoder dat is ontworpen om ruis uit de invoergegevens te verwijderen. Het is een populaire techniek in deep learning voor het voorverwerken van gegevens, het leren van robuuste representaties, en voor compressiedoeleinden.\n",
    "\n",
    "In deze oefening ga je een denoising autoencoder implementeren in PyTorch. Deze oefening omvat de volgende stappen:\n",
    "\n",
    "* Data voorbereiden: We gebruiken de Fashion MNIST dataset, voegen ruis toe aan de afbeeldingen en schalen de gegevens naar het bereik [0, 1]. (Dit deel krijg je hieronder)\n",
    "* Model bouwen: We bouwen een eenvoudig denoising autoencoder model met een encoder en een decoder.\n",
    "* Model trainen: We trainen het model met ruisachtige afbeeldingen als invoer en niet-vervuilde afbeeldingen als doel.\n",
    "* Resultaten evalueren: We testen het model door enkele ruisachtige afbeeldingen door het netwerk te laten gaan en hun gereconstrueerde versies weer te geven."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c51aee-5ebb-466c-9a9c-cdecdffa410b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data voorbereiden\n",
    "\n",
    "# Importeren van benodigde bibliotheken\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Controleer of er een GPU beschikbaar is, zo niet gebruik de CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Data-transformatie: normaliseer de afbeeldingen zodat de pixelwaarden tussen 0 en 1 liggen\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()  # Converteert beeld naar tensor en schaalt automatisch naar [0, 1]\n",
    "])\n",
    "\n",
    "# FashionMNIST dataset downloaden en laden\n",
    "train_dataset = datasets.FashionMNIST(root='./data', train=True, transform=transform, download=True)\n",
    "test_dataset = datasets.FashionMNIST(root='./data', train=False, transform=transform, download=True)\n",
    "\n",
    "# DataLoader voor batches van de trainings- en testdata\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "print(\"Data geladen en DataLoader klaar.\")\n",
    "\n",
    "# Functie om ruis aan de afbeeldingen toe te voegen\n",
    "def add_noise(imgs, noise_factor=0.5):\n",
    "    noisy_imgs = imgs + noise_factor * torch.randn_like(imgs)\n",
    "    noisy_imgs = torch.clamp(noisy_imgs, 0., 1.)\n",
    "    return noisy_imgs\n",
    "\n",
    "# Visualisatie van enkele ruisachtige afbeeldingen\n",
    "examples = enumerate(test_loader)\n",
    "batch_idx, (example_data, _) = next(examples)\n",
    "\n",
    "noisy_imgs = add_noise(example_data)\n",
    "\n",
    "fig, axes = plt.subplots(1, 5, figsize=(15, 4))\n",
    "for i in range(5):\n",
    "    axes[i].imshow(noisy_imgs[i].squeeze().numpy(), cmap='gray')\n",
    "    axes[i].axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235363bd-2559-49a3-8a2a-6045643b1e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model bouwen\n",
    "class DenoisingAutoencoder(nn.Module):\n",
    "    def __init__(self, ):\n",
    "        super(DenoisingAutoencoder, self).__init__()\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1), # aantal in kanalen is 1 want grijswaardenbeelden\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1), # 32 * 7 * 7\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=7, stride=1, padding=0) # 64 * 1 * 1\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=7, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 16, kernel_size=3, stride=2, padding= 1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(16, 1, kernel_size=3, stride=2, padding= 1, output_padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x  = self.encoder(x)\n",
    "        return self.decoder(x)\n",
    "\n",
    "model = DenoisingAutoencoder().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4c5b8f-c716-4feb-aa5d-98047c2e213f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model trainen\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0\n",
    "    for images, _ in train_loader:\n",
    "        images = images.to(device)\n",
    "        images_noise = add_noise(images).to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        images_de = model(images_noise)\n",
    "        loss = criterion(images_de, images)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch}/{num_epochs}, loss: {running_loss/len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591ce60f-3f5b-4e90-8d58-2710efbd2fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resultaten visualiseren\n",
    "\n",
    "model.eval()\n",
    "test_examples = None\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        imgs, _ = batch\n",
    "        noisy_imgs = add_noise(imgs).to(device)\n",
    "        outputs = model(noisy_imgs)\n",
    "        test_examples = (imgs, noisy_imgs, outputs)\n",
    "        break\n",
    "\n",
    "# Plot de resultaten\n",
    "orig, noisy_imgs, outputs = test_examples\n",
    "fig, axes = plt.subplots(3, 5, figsize=(15, 6))\n",
    "for i in range(5):\n",
    "    # Originele afbeeldingen\n",
    "    print(orig[i].shape)\n",
    "    axes[0, i].imshow(orig[i].cpu().squeeze().numpy(), cmap='gray')\n",
    "    axes[0, i].axis('off')\n",
    "\n",
    "    # Ruisachtige afbeeldingen\n",
    "    axes[1, i].imshow(noisy_imgs[i].cpu().squeeze().numpy(), cmap='gray')\n",
    "    axes[1, i].axis('off')\n",
    "    \n",
    "    # Gereconstrueerde afbeeldingen\n",
    "    axes[2, i].imshow(outputs[i].cpu().squeeze().numpy(), cmap='gray')\n",
    "    axes[2, i].axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8ce740-c79d-4630-bc16-53dc730226e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
