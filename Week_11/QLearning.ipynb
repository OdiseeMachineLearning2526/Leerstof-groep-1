{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_iCCMpkHqd_R"
   },
   "source": [
    "# Reinforcement learning - 4 op een rij\n",
    "\n",
    "Reinforcement learning komt uit de studie van Markov Chains of Processen voor.\n",
    "Dit is een random opeenvolging van states waarbij elke transisitie een mogelijke kans heeft.\n",
    "Door een reward te koppelen aan elke state waarin je komt kan je een functie opstellen die de de totale reward maximaliseert.\n",
    "Dit is het basisidee achter reinforcement learning.\n",
    "\n",
    "Een aantal belangrijke termen/concepten hierbij zijn:\n",
    "* De agent\n",
    "* Het environment\n",
    "* De state space\n",
    "* De action space\n",
    "* De reward en return\n",
    "* Exploration vs exploitation\n",
    "\n",
    "## Q-learning\n",
    "\n",
    "Een eerste algoritme dat we bekijken voor reinforcement learning uit te voeren is Q-learning.\n",
    "Dit algoritme maakt gebruik van de Q-functie of action-value function.\n",
    "Hiervoor houdt het Q-learning algoritme een matrix bij dat de reward van actie in een state bepaald.\n",
    "In een verkenningsfase laten we toe dat er sub-optimale keuzes genomen worden.\n",
    "Nadat dit lang genoeg gerund heeft, gaan we over naar een exploitation fase waarbij enkel de beste keuzes genomen worden.\n",
    "\n",
    "Om te tonen hoe je het Q-learning algoritme kan implementeren, kan je gebruik maken van het [gymnasium package](https://gymnasium.farama.org/).\n",
    "Dit bevat heel wat eenvoudige environments van spelletjes in python die hiervoor gebruikt kunnen worden.\n",
    "\n",
    "In onderstaande code gaan we een AI-model maken om vier op een rij te spelen.\n",
    "Het environment (met reeds een aantal ingebouwde AI-agents) kan je [hier](https://github.com/lucasBertola/Connect-4-Gym-env-Reinforcement-learning/tree/main) vinden.\n",
    "In onderstaande code-cell toon ik een demo van hoe je zelf tegen een Agent 4 op een rij kan spelen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from connect_four_gymnasium import ConnectFourEnv\n",
    "from connect_four_gymnasium.players import SelfTrained6Player\n",
    "from connect_four_gymnasium.players import ConsolePlayer\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "you = ConsolePlayer()\n",
    "env = ConnectFourEnv(opponent= SelfTrained6Player(deteministic=True), render_mode=\"rgb_array\",main_player_name=\"you\")\n",
    "\n",
    "obs , _=  env.reset()\n",
    "for i in range(5000):\n",
    "    \n",
    "    clear_output(wait=True)\n",
    "    frame = env.render()\n",
    "    plt.imshow(frame)\n",
    "    plt.axis('off')\n",
    "    display(plt.gcf())\n",
    "    plt.close()  # close the figure to avoid memory buildup\n",
    "    \n",
    "    action = you.play(obs)\n",
    "    obs, rewards, done, truncated,info = env.step(action)\n",
    "\n",
    "    print(obs)\n",
    "    print(rewards)\n",
    "    \n",
    "    if(truncated or done):\n",
    "        if rewards > 0:\n",
    "            print(\"player won\")\n",
    "            break\n",
    "        else:\n",
    "            print(\"player lost\")\n",
    "            break\n",
    "        obs , _=  env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zoals reeds eerder besproken hebben we bij reinforcement learning hebben we de volgende zaken nodig om een AI model te trainen:\n",
    "* De mogelijke acties die het AI-model kan nemen (action space)\n",
    "* De state van de environment dat gedetecteerd wordt door het model (observation space)\n",
    "* De beloningen uitgestuurd door het systeem\n",
    "\n",
    "Wat deze zaken voorstellen en welke structuur deze waarden hebben hangt sterk af van de specifieke omgeving en kan dus vaak opgezocht worden in de documentatie van de omgeving.\n",
    "Door het uitprinten van de observatie en de reward hebben we reeds een indicatie van wat de omgeving/environment bijhoudt als interne state en uitstuurt als beloning.\n",
    "\n",
    "In dit geval hebben we de volgende kenmerken:\n",
    "* action space: een getal tussen 1 en 7 (welke kolom je een stuk inplaatst)\n",
    "* observation space: een matrix van 6x7 met 0 als de plaats vrij is, 1 en -1 als respectievelijk speler 1 of speler 2 er een munt geplaatst heeft\n",
    "* rewards: 0 als het spel bezig is, 1 als speler 1 gewonnen heeft (of speler 2 een ongeldige zet gedaan heeft) en -1 als speler 2 gewonnen heeft (of speler 1 een ongeldige zet gedaan heeft).\n",
    "\n",
    "## Oefening - Random agent\n",
    "\n",
    "Schrijf een AI-agent die random zetten neemt. Hiervoor schrijf je een klasse RandomAgent die een subklasse is van de base-klasse Player in de library.\n",
    "De base klasse heeft de volgende structuur:\n",
    "```python\n",
    "class Player:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "\n",
    "    def play(self, observation):\n",
    "        raise NotImplementedError(\"The 'play' method must be implemented in the child class\")\n",
    "\n",
    "    def getElo(self):\n",
    "        return None\n",
    "    \n",
    "    def getName(self):\n",
    "        return self.name\n",
    "\n",
    "    def isDeterministic(self):\n",
    "        raise NotImplementedError(\"The 'isDeterministic' method must be implemented in the child class\")\n",
    "```\n",
    "\n",
    "Zorg er daarna voor dat je een spel kan spelen tegen je eigen geschreven agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Op dit moment kunnen we deze agent een aantal keer 4 op een rij laten spelen tegen een andere speler.\n",
    "Doe dit hieronder en bekijk het winstpercentage tegen verschillende opponenten.\n",
    "Ga in de documentatie op zoek naar een aantal tegenstanders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Van random agent naar lerende agent\n",
    "\n",
    "Maak nu een andere agent die het Q-learning algoritme implementeert.\n",
    "Hiervoor zorgen we dus voor de volgende zaken:\n",
    "* Zorg dat er een dictionary is voor (state, action) -> q_value\n",
    "* De gewenste actie voor een gegeven state/observation is dan degene met de hoogste waarde in de dictionary. Indien er meerdere eenzelfde waarde hebben kan je kiezen\n",
    "* Voeg een learn functie toe dat de reward update volgens de functie waarbij alpha de learning_rate is en gamma de discount_factor\n",
    "```\n",
    "        Q(s,a) ← Q(s,a) + α [ r + γ max_a' Q(s',a') – Q(s,a) ]\n",
    "```\n",
    "* Voeg een epsilon-greedy aanpak toe in het nemen van de actie om met een bepaalde factor een random waarde te kiezen en niet de beste\n",
    "* Voeg een set_deterministic functie toe om de episolon-waarde op 0 te zetten zodat er geen exploration meer toegelaten wordt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nu moeten we een leerproces maken. Hiervoor gaan we onze Q-learning agent laten spelen.\n",
    "Hier zijn verschillende opties voor  \n",
    "\n",
    "1. **Agent traint tegen zichzelf (self-play)**\n",
    "\n",
    "✅ Voordeel: de agent leert steeds sterker spelen, omdat hij moet verbeteren om zichzelf te verslaan.\n",
    "\n",
    "❌ Nadeel: kan instabiel zijn → beide kanten maken dezelfde fouten in het begin, en de agent kan \"domme\" strategieën aanleren die in echte tegenstand niet werken.\n",
    "\n",
    "Self-play werkt vaak beter als je af en toe het oude beleid (policy) behoudt, of een mix doet (bijvoorbeeld: soms random, soms het huidige beleid).\n",
    "\n",
    "2. **Agent traint tegen een RandomPlayer**\n",
    "\n",
    "✅ Voordeel: simpele manier om te starten, de agent leert al snel dat domme zetten verliezen opleveren.\n",
    "\n",
    "✅ Makkelijk om eerste progressie te zien in de Q-waarden.\n",
    "\n",
    "❌ Nadeel: na een tijdje leert de agent vooral exploiteren dat de tegenstander random speelt, en leert niet per se een sterke algemene strategie.\n",
    "\n",
    "3. **Agent traint tegen een bestaande sterke speler (bv. je SelfTrained6Player)**\n",
    "\n",
    "✅ Voordeel: de agent wordt gedwongen om tegen \"echte\" strategieën te leren.\n",
    "\n",
    "❌ Nadeel: in het begin verliest hij bijna alles → weinig beloning → learning kan traag of zelfs falen.\n",
    "→ vaak los je dit op door curriculum learning: eerst tegen RandomPlayer, dan tegen sterkere tegenstanders.\n",
    "\n",
    "Wij gaan de laatste methode toepassen waarbij opvolgend de BabySmarterPlayer, de ChildPlayer, de ChildSmarterPlayer, de TeenagerPlayer. de TeenagerSmarterPlayer. de AdultPlayer en de AdultSmarterPlayer gebruikt worden.\n",
    "Elke keer dat de ai 5 opeenvolgende keren wint van de tegenstander gaat hij over naar de volgende moeilijkheid.\n",
    "In het geval hij tegen iedereen gewonnen is, dan speelt hij verder tegen zichzelf.\n",
    "Schrijf nu een trainingslus die dit uitvoert en train de agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Speel nu zelf nog eens tegen de agent, hoe ervaar je het?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RL in neural networks\n",
    "\n",
    "Het gebruik van Q-learning werkt goed als het aantal states en acties beperkt zijn.\n",
    "Dit is echter zelden het geval, denk bijvoorbeeld aan een continue variabele zoals snelheid of locatie.\n",
    "\n",
    "Een oplossing hiervoor is om de action-value functie die in Q-learning geoptimaliseerd wordt te benaderen ipv exact te berekenen.\n",
    "Dit kan bijvoorbeeld door middel van een neuraal netwerk te gebruiken.\n",
    "Er zijn verschillende model-structuren die hiervoor ontwikkeld zijn zoals:\n",
    "- DQN (onderwerp van onderstaande demo)\n",
    "- REINFORCE\n",
    "- DDPG\n",
    "- TD3\n",
    "- PPO\n",
    "- SAC\n",
    "\n",
    "Voor we beginnen met het uitwerken van een model.\n",
    "Bekijk [deze tutorial](https://www.tensorflow.org/agents/tutorials/1_dqn_tutorial) en beantwoord de volgende vragen:\n",
    "- Wat is de state en wat zijn de mogelijke acties?\n",
    "- Wat is de structuur van het gebruikte DQN?\n",
    "- Zijn er nieuwe hyperparameters gebruikt?\n",
    "- Welke metriek wordt er gebruikt en waar wordt deze berekend?\n",
    "- Hoe worden de gewichten aangepast?\n",
    "- Waarvoor wordt de ReplayBuffer gebruikt?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Antwoord:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Schrijf nu zelf de nodige code om het DQN-model toe te passen op het \"4 op een rij\" environment van hierboven."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Speel tenslotte nog eens zelf tegen je agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "d5e8e3a19af5ceb2434683dff87da6345c3b29f7eb0a8a138558c07d014a01cc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
